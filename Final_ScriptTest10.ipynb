{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3fd59f",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "# Commodity Risk Management\n",
    "### Asymmetric Volatility Forecasting & Dynamic VaR\n",
    "*GJR-GARCH Approach*\n",
    "\n",
    "---\n",
    "\n",
    "> **The Core Observation:** ARCH/GARCH models are built on the idea that volatility spikes suddenly and persists before reverting to its long-term mean. This creates a phenomenon known as *volatility clustering*.\n",
    "\n",
    "> **Mechanics:** In the GARCH framework:\n",
    "> * $u_{t-1}^2$ represents the **shock** generating volatility.\n",
    "> * The $\\beta$ (Beta) coefficient captures the **persistence** of volatility after a shock.\n",
    "\n",
    "*(Extensions included: Exponential GARCH, asymmetric models, regime-switching, etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ad3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0358fe",
   "metadata": {},
   "source": [
    "## 01. Data Extraction & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download('BNO', start = \"2015-01-01\", end = '2025-11-27')\n",
    "\n",
    "data.columns = data.columns.droplevel(1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fca4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = data.Close\n",
    "\n",
    "returns = np.log(prices/prices.shift(1))\n",
    "\n",
    "data['Returns'] = returns\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024ea0c",
   "metadata": {},
   "source": [
    "## 02. Descriptive Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.plot(figsize=(16,9), color = 'red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Returns')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613223f",
   "metadata": {},
   "source": [
    "> **Visual inspection of the Brent log-return series reveals four key stylized facts:**\n",
    ">\n",
    "> 1. **Stationarity & Mean Reversion ðŸ”„**\n",
    ">    Unlike the raw price series which exhibits a stochastic trend, the return series oscillates around a constant mean close to zero. This stationarity is crucial for modeling.\n",
    ">\n",
    "> 2. **Volatility Clustering (Heteroscedasticity) ã€°ï¸**\n",
    ">    The plot illustrates Mandelbrot's observation: *\"Large changes tend to be followed by large changes.\"* We see distinct regimes of calm (2017) vs. turbulence (2020, 2022).\n",
    ">\n",
    "> 3. **Impact of Exogenous Shocks ðŸ’¥**\n",
    ">    Extreme spikes correspond to real-world events like the **COVID-19 oil crash** (-25% drop) or the 2022 geopolitical tensions.\n",
    ">\n",
    "> 4. **Leptokurtosis & \"Fat Tails\" ðŸ””**\n",
    ">    The distribution significantly deviates from a Normal Law. Extreme events happen far more frequently than a Gaussian model would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27423904",
   "metadata": {},
   "outputs": [],
   "source": [
    "(returns ** 2).plot(figsize=(16,9), color = 'red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Squared Returns')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf83b81",
   "metadata": {},
   "source": [
    "> **ðŸ“‰ Squared Returns ($r_t^2$)**\n",
    ">\n",
    "> They are essential because they constitute the best observable proxy for the *unobservable* conditional variance (volatility) of returns at a given time.\n",
    ">\n",
    "> *(Since daily returns usually have a mean very close to zero ($\\mu \\approx 0$), the squared return acts as a direct approximation of the variance.)*\n",
    ">\n",
    "> 1. **Visualizing Volatility Clustering ðŸ‘ï¸**\n",
    ">    They allow for a clear visualization of volatility clustering (regrouping of high-variance periods) which might be less obvious in raw returns.\n",
    ">\n",
    "> 2. **Model Input (Variance Equation) ðŸ“**\n",
    ">    They serve as the dependent variable (the input) for the variance equation within the GARCH model structure.\n",
    ">\n",
    "> 3. **Statistical Testing (ARCH-LM / Ljung-Box) ðŸ§ª**\n",
    ">    They are the subject of the **Ljung-Box** or **ARCH-LM test**. If $r_t^2$ are significantly autocorrelated, it proves that volatility has temporal dependence and that a GARCH model is statistically necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78969118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_clean = returns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljung_box = acorr_ljungbox(returns_clean**2., lags = 10, return_df = True)\n",
    "\n",
    "print(ljung_box.apply(lambda x: x.map('{:.4f}'.format)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873e454",
   "metadata": {},
   "source": [
    "## 03. Conditional Mean Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221338ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "q = 1\n",
    "model = ARIMA(returns,order=(p,0,q))\n",
    "arma = model.fit()\n",
    "\n",
    "print(arma.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c3775",
   "metadata": {},
   "source": [
    "> **Hypothesis Testing: The Zero-Mean Assumption**\n",
    ">\n",
    "> 1. **Statistical Evidence ðŸ“‰**\n",
    ">    The sample mean is extremely low (approx. 0.000009). More importantly, the **p-value > 0.05** implies that we fail to reject the null hypothesis ($H_0: \\mu = 0$). The mean is statistically indistinguishable from zero.\n",
    ">\n",
    "> 2. **Modeling Implication ðŸŽ¯**\n",
    ">    Since $\\mu \\approx 0$, we can simplify the conditional mean equation. The return $r_t$ is approximately equal to the residual (innovation) term:\n",
    ">\n",
    ">    $$r_t \\approx \\epsilon_t$$\n",
    ">\n",
    ">    This allows us to focus directly on modeling the variance (GARCH) without fitting a complex ARIMA model for the mean first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191dfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = arma.resid\n",
    "\n",
    "residuals_clean = residuals.dropna()\n",
    "\n",
    "residuals_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5b8dd",
   "metadata": {},
   "source": [
    "## 04. GARCH vs. GJR-GARCH Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "am = arch_model(residuals_clean*100, mean = 'Constant', vol = 'Garch', p = 1, q = 1, dist='Normal' )\n",
    "\n",
    "res_garch = am.fit(disp='off')\n",
    "\n",
    "print(res_garch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ae273",
   "metadata": {},
   "outputs": [],
   "source": [
    "gjr = arch_model(residuals_clean*100, mean = 'Constant', vol = 'GARCH', p = 1, o = 1, q = 1, dist = 't')\n",
    "res_gjr = gjr.fit(disp='off')\n",
    "print(res_gjr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c572",
   "metadata": {},
   "source": [
    "### ðŸ† Model Selection: Statistical Comparison\n",
    "\n",
    "| Metric | Target | GARCH(1,1) | GJR-GARCH(1,1) |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| **Log-Likelihood** | (Maximize â†‘) | -5864.30 | **-5785.12** |\n",
    "| **AIC** | (Minimize â†“) | 11736.60 | **11582.25** |\n",
    "| **BIC** | (Minimize â†“) | 11760.26 | **11617.75** |\n",
    "\n",
    "> **âœ… Conclusion:**\n",
    ">\n",
    "> The **GJR-GARCH** model is superior. It exhibits a higher *Log-Likelihood* (better fit) and lower Information Criteria (AIC & BIC), indicating that the inclusion of the asymmetry parameter justifies the slight increase in model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb3570",
   "metadata": {},
   "source": [
    "## 05. Statistical Validation & Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c143cb",
   "metadata": {},
   "source": [
    "> **ðŸ” Diagnostics on Standardized Residuals ($z_t$)**\n",
    ">\n",
    "> To validate the GJR-GARCH model, we must inspect the standardized residuals, defined as:\n",
    ">\n",
    "> $$z_t = \\frac{\\epsilon_t}{\\sigma_t}$$\n",
    ">\n",
    "> 1. **No Autocorrelation (White Noise) ðŸ“‰**\n",
    ">    The standardized residuals should behave like white noise. We use the **Ljung-Box Test** to confirm that no linear dependence remains.\n",
    ">\n",
    "> 2. **No Remaining ARCH Effects ðŸ§¬**\n",
    ">    We perform the **ARCH-LM Test** on squared standardized residuals ($z_t^2$) to ensure the model has successfully captured all volatility clustering.\n",
    ">\n",
    "> 3. **Distribution Fit ðŸ””**\n",
    ">    We compare the distribution of $z_t$ against the assumed distribution (Student-t) using a Q-Q Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e386569",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_gjr = res_gjr.std_resid\n",
    "\n",
    "residuals_gjr_squared = residuals_gjr ** 2\n",
    "\n",
    "ljung_box_gjr = acorr_ljungbox(residuals_gjr_squared, lags = 10, return_df= True)\n",
    "print(ljung_box_gjr.apply(lambda x: x.map('{:.4f}'.format)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b92f5d",
   "metadata": {},
   "source": [
    "> **âœ… Validation Success: Absence of ARCH Effects**\n",
    ">\n",
    "> 1. **Statistical Evidence (p-values > 0.05) ðŸ“Š**\n",
    ">    All p-values from the ARCH-LM test exceed the 5% significance level. Therefore, we cannot reject the Null Hypothesis ($H_0$: no ARCH effects).\n",
    ">\n",
    "> 2. **Interpretation ðŸ§¹**\n",
    ">    There is no remaining autocorrelation in the squared standardized residuals. The GJR-GARCH model has successfully **\"cleaned\"** all conditional heteroscedasticity from the data.\n",
    ">\n",
    "> 3. **Conclusion ðŸ**\n",
    ">    The residuals are now **homoscedastic** (they behave like white noise regarding variance). The model is statistically valid and robust for forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbdf67",
   "metadata": {},
   "source": [
    "## 06. Volatility Forecasting & Value-at-Risk (VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca6cf7",
   "metadata": {},
   "source": [
    "> **ðŸ”® From Volatility to Risk Management: Dynamic VaR**\n",
    ">\n",
    "> 1. **The Concept ðŸ›¡ï¸**\n",
    ">    Value-at-Risk (VaR) estimates the maximum potential loss over a specific time horizon at a given confidence level (e.g., 95%). Unlike static methods, our **Dynamic VaR** adjusts daily based on the GJR-GARCH volatility forecast.\n",
    ">\n",
    "> 2. **The Formula ðŸ“**\n",
    ">    Since the mean is negligible ($\\mu \\approx 0$), the One-Day VaR at significance level $\\alpha$ (e.g., 5%) is calculated as:\n",
    ">\n",
    ">    $$\\text{VaR}_t = \\sigma_t \\times Q_\\alpha$$\n",
    ">\n",
    ">    Where:\n",
    ">    * $\\sigma_t$ : Conditional volatility forecast by GJR-GARCH.\n",
    ">    * $Q_\\alpha$ : The quantile of the Student-t distribution (capturing fat tails).\n",
    ">\n",
    "> 3. **Interpretation ðŸ’¡**\n",
    ">    If the actual return falls below the VaR line, it is considered a **\"VaR Breach\"** (or exception). This signals an extreme market event that the model predicted with only $\\alpha$ probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f480104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = yf.download('BNO', start = '2025-11-27', end = '2025-12-05')\n",
    "\n",
    "prices_test = data_test.Close\n",
    "\n",
    "returns_test = np.log(prices_test/prices_test.shift(1))\n",
    "\n",
    "data_test['Returns Test'] = returns_test\n",
    "\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "horizon_days = 5\n",
    "forecasts = res_gjr.forecast(horizon=horizon_days)\n",
    "\n",
    "pred_vol_series = np.sqrt(forecasts.variance.iloc[-1].values)\n",
    "\n",
    "real_returns_series = data_test['Returns Test'].dropna()\n",
    "\n",
    "limit = min(len(real_returns_series), horizon_days)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Date': real_returns_series.index[:limit],\n",
    "    'Real Return (%)': real_returns_series.values[:limit] * 100,\n",
    "    'Forecasted Volatility (%)': pred_vol_series[:limit]\n",
    "})\n",
    "\n",
    "nu = res_gjr.params['nu']\n",
    "t_quantile = stats.t.ppf(0.01, nu)\n",
    "comparison['VaR 99% (%)'] = comparison['Forecasted Volatility (%)'] * t_quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f874c8",
   "metadata": {},
   "source": [
    "> ### ðŸ Final Test: Forecast vs. Reality (Backtest)\n",
    ">\n",
    "> | Date | Real Return (%) | Forecasted Vol (%) | VaR 99% (%) | Status |\n",
    "> | :--- | :---: | :---: | :---: | :---: |\n",
    "> | 2025-12-01 | +0.068 % | 1.751 % | **-5.351 %** | âœ… Safe |\n",
    "> | 2025-12-02 | -1.241 % | 1.764 % | **-5.391 %** | âœ… Safe |\n",
    "> | 2025-12-03 | +0.508 % | 1.777 % | **-5.421 %** | âœ… Safe |\n",
    "> | 2025-12-04 | +0.721 % | 1.709 % | **-5.465 %** | âœ… Safe |\n",
    ">\n",
    "> *(*Safe* implies that the Real Return > VaR 99%. No breach occurred during this period.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ad1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conditional_vol = res_gjr.conditional_volatility \n",
    "\n",
    "returns_hist = residuals_clean * 100 \n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(returns_hist.index, returns_hist, color='blue', alpha=0.4, lw=1, label='Brent Daily Returns')\n",
    "\n",
    "plt.plot(conditional_vol.index, conditional_vol, color='red', lw=1.5, label='Estimated Volatility (GJR-GARCH)')\n",
    "plt.plot(conditional_vol.index, -conditional_vol, color='red', lw=1.5)\n",
    "\n",
    "plt.title('Brent Volatility Modeling : GJR-GARCH(1,1) Student', fontsize=16)\n",
    "plt.ylabel('Returns / Volatility (%)', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619baf8c",
   "metadata": {},
   "source": [
    "## 07. Backtesting & Final Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eec1e2",
   "metadata": {},
   "source": [
    "> **ðŸ Final Test: Forecast vs. Reality**\n",
    ">\n",
    "> 1. **Out-of-Sample Backtesting ðŸ“…**\n",
    ">    We compare the model's predictions (*ex-ante*) against the actual market data (*ex-post*) for the most recent trading days. This is the ultimate test of the model's predictive power.\n",
    ">\n",
    "> 2. **VaR Breach Check ðŸš¨**\n",
    ">    We monitor if the **Real Return** falls below the **VaR 99%** threshold.\n",
    ">    * If *Return* > *VaR*: The risk was correctly covered. âœ…\n",
    ">    * If *Return* < *VaR*: It is a **Breach** (Exception). Too many breaches would invalidate the model.\n",
    ">\n",
    "> 3. **Visual Fit ðŸ“ˆ**\n",
    ">    The final plot illustrates the **Vol-Envelope**. The red line (Estimated Volatility) should tightly hug the blue spikes (Returns), expanding during crises and contracting during calm periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['VaR Validation'] = np.where(\n",
    "    comparison['Real Return (%)'] < comparison['VaR 99% (%)'], \n",
    "    'âŒ EXCEPTION',  \n",
    "    'âœ… OK'       \n",
    ")\n",
    "\n",
    "comparison['Score Sigma'] = comparison['Real Return (%)'] / comparison['Forecasted Volatility (%)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8783291",
   "metadata": {},
   "source": [
    "### ðŸ Backtesting Results: VaR Breach Analysis\n",
    "\n",
    "| Date | Real Return | Forecast Vol | VaR 99% | Score $\\sigma$ | Validation |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| 2025-12-01 | +0.069 % | 1.752 % | **-5.351 %** | 0.039 | âœ… OK |\n",
    "| 2025-12-02 | -1.241 % | 1.765 % | **-5.391 %** | -0.703 | âœ… OK |\n",
    "| 2025-12-03 | +0.508 % | 1.777 % | **-5.421 %** | 0.331 | âœ… OK |\n",
    "| 2025-12-04 | +0.722 % | 1.789 % | **-5.465 %** | 0.403 | âœ… OK |\n",
    "\n",
    "> * **Definition:** Score $\\sigma$ measures the return standardized by the forecasted volatility. A score outside [-3, +3] would indicate an extreme event.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd05bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "volatility = res_gjr.conditional_volatility\n",
    "nu = res_gjr.params['nu']\n",
    "t_quantile = stats.t.ppf(0.01, nu)\n",
    "VaR_series = volatility * t_quantile\n",
    "\n",
    "def kupiec_pof_test(returns, var_series, confidence_level=0.99):\n",
    "    alpha = 1 - confidence_level\n",
    "    N = len(returns)\n",
    "    breaches = returns < var_series\n",
    "    x = np.sum(breaches)\n",
    "    p = x / N\n",
    "\n",
    "    if x == 0:\n",
    "        print(\"Aucune brÃ¨che observÃ©e ! Le modÃ¨le est (trop) prudent.\")\n",
    "        return 0, 1.0, x, N\n",
    "    \n",
    "    numerator = ((1 - alpha)**(N - x)) * (alpha**x)\n",
    "    denominator = ((1 - p)**(N - x)) * (p**x)\n",
    "    \n",
    "    lr_stat = -2 * np.log(numerator / denominator)\n",
    "    p_value = 1 - stats.chi2.cdf(lr_stat, df=1)\n",
    "    \n",
    "    return lr_stat, p_value, x, N\n",
    "\n",
    "lr, p_val, failures, total = kupiec_pof_test(residuals_clean * 100, VaR_series, confidence_level=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687fdf2",
   "metadata": {},
   "source": [
    "> ### âŒ Kupiec Test Diagnosis: Model Validation Failed\n",
    ">\n",
    "> *The Kupiec test indicates that the model is **too conservative**. The observed number of exceptions (breaches) is significantly lower than the theoretical expectation for a 99% VaR.*\n",
    ">\n",
    "> **ðŸ“‰ The Diagnosis:**\n",
    "> * **Observed Failures:** 0.62% (vs 1.00% expected).\n",
    "> * **Implication:** The calculated VaR overestimates the risk. It is \"too high\" and rarely breached.\n",
    "> * **Result:** Reject $H_0$ (p-value 0.0315 < 0.05).\n",
    ">\n",
    "> **ðŸ› ï¸ Strategic Fixes (Recalibration Roadmap):**\n",
    "> 1. **Parameter Tuning:** Adjust GJR-GARCH coefficients or the Student-t degrees of freedom (DoF) to better reflect observed losses.\n",
    "> 2. **VaR Scaling:** Apply a slight multiplicative factor (< 1) to lower the VaR without altering confidence levels, aligning exceptions with theory.\n",
    "> 3. **Distribution Selection:** Adopt asymmetric distributions (e.g., Skewed Student-t) or semi-parametric methods (Cornish-Fisher) to better capture skewness and fat tails.\n",
    "> 4. **Iterative Backtesting:** Re-calculate and test iteratively until the model satisfies the Kupiec condition (p-value > 0.05)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
